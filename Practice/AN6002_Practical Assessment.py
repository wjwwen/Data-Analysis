# -*- coding: utf-8 -*-
"""AN6002.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SkvFqOg5HJN805xTuunddxOyzxoExFv8

#### **Week 2 Workshop - Employee** -----------------------------------------
"""

import pandas as pd
df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/employee.csv")

# Lesson 2 Activity 2
# How many records are there in this data file?
print(f"Q1-> {df.shape[0]}")

# How many columns are there in the data file?
print(f"Q2-> {df.shape[1]}")

# Any duplicated records? How many?
Q3a = "Yes" if True in df.duplicated() else "No" # assign Yes if duplicated record exists.
print(f"Q3a-> {Q3a}")
print(f"Q3b-> {df.duplicated().sum()}")

# Any records with the same EmployeeNumber? How many? 
Q4a = "Yes" if any(df.duplicated(subset="EmployeeNumber")) else "No" # assign Yes if duplicated EmployeeNumber record exists.
print(f"Q4a-> {Q4a}")
print(f"Q4b-> {df.EmployeeNumber.duplicated().sum()}")

# Are there any null records? How many?
Q5a = "Yes" if any(df.isnull()) else "No" # assign Yes if duplicated EmployeeNumber record exists.
print(f"Q5a-> {Q5a}")
print(f"Q5b-> {df.isna().sum().sum()}")

### Q1 Adds in prefix of “E” in front of each EmployeeNumber
df['EmployeeNumber'] = "E" + df.EmployeeNumber.astype(str)

### Q2 Replace WorkLifeBalance column with text from header file, e.g. 1 with bad
df.WorkLifeBalance.replace([1, 2, 3, 4], ['Bad', 'Good', 'Better', 'Best'], inplace=True)

### Q3 Examine MonthlyRate and MonthIncome, any issue? 
# Both should be numeric, but MonthIncome is Object type instead hinting to data issue.
df.MonthlyRate.dtype, df.MonthlyIncome.dtype

df_problem = df[df.MonthlyIncome.str.isnumeric() == False]
# EnvironmentSatisfaction column and onwards is lagged by 1 column

#.loc[:, "EnvironmentSatisfaction":]
df_problem

# Keep original columns until EmployeeCount for problematic records
df1 = df_problem.loc[:, :"EmployeeCount"]

# EmployeeNumber columns onwards shift left by one
df2 = df_problem.shift(-1, axis=1).loc[:,"EmployeeNumber":]

# Merge the above columns
df1.merge(df2, how='inner',left_index=True, right_index=True)

# Replace the original records with merged records
df.loc[[3807,6146],:] = df1.merge(df2, how='inner',left_index=True, right_index=True)

df.loc[[3807,6146],:]

# Record 3807 still contains error on Department etc., drop it accordingly.
df.drop(index=3807, inplace=True)

# Activity 4 - Messy salary data
df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/messy.csv")
df

df.columns

# Replace blanks with 
import numpy as np
print(df.replace(r'^\s*$', np.nan, regex=True))

# Replace values within a column
df['CompanyType'] = df['CompanyType'].replace(['111'], 'Medical')
df['CompanyType'] = df['CompanyType'].replace(['222'], 'Engineering')
df['CompanyType'] = df['CompanyType'].replace(['333'], 'Goods')
 

# Replace NaN
df['Salary'] = df['Salary'].fillna('0') 
df['Bonus'] = df['Bonus'].fillna('0') 
df

"""#### **Week 3 Workshop: Population Growth*** ------------------------- """

# Task 1
import seaborn as sns
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/populationGrowth.csv")
sns.lmplot(data=df, x="Year", y="Growth", hue="Category", fit_reg=False)

# Task 2  
import pandas as pd
import seaborn as sns
df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/supermarket_data.csv")
# df.head()
df['total_revenue'] = df.iloc[:,-7:-1].sum(axis = 1)
sns.barplot(data=df, y="total_revenue",x="Distribution Channel",ci = None, estimator = sum)
#  Ans: Distribution Channel 1

df2=df.iloc[:,-7:-1].sum(axis = 0)
df2.plot(kind="bar")
# Ans: Cold Food

## Task 4
sns.pairplot(df)

"""####**Week** 4 Workshop - Commodity A, B, C"""

# Task P1
import pandas as pd
import seaborn as sns

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/commodity_a.csv")

# Average (mean)
print(f"Q1-> ${df.Gold.mean():.2f}")
print(f"Q2-> ${df.Silver.mean():.2f}")
print(f"Q3-> ${df.Platinum.mean():.2f}")

# Which metal has a larger spread
print(f"Q4-> \n{df.iloc[:,1:].std().sort_values(ascending=False)}")

"""<h2>Task P2 – Absentees Analysis - With additional questions on time series</h2>

1. Present the numerical and graphical summary for the given variables.
2. What observations can you see from the above information?
3. What is the period cover in this dataset?
4. Pair up the variables and plot it graphically.
5. Examine any association between the variables.
6. What is the most common reason for absentees?
7. Which season do we see the highest absentee rates?
8. Which month do we see the highest absentee rates?
9. Which day of the week do we see the highest absentee rates?
10. What is the median hours of absence for the most common reason of absence from this dataset?
11. Do you think BMI plays a part in this issue of absence from work? Based on what evidence?
12. Any significant trend or seasonality you can observe from this absentees dataset.
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/absentees.csv",
                 parse_dates=["Date"], index_col="Date")

df.index = df.index.tz_localize('Brazil/Acre') # localize it to Brazil timezone, optional step

df.columns

df.rename(columns={"Reason for Absence": "Reason", 
                   "Transportation Expense": "TransportExp",
                   "Distance to Work": "Distance",
                   "Service time": "ServiceTime",
                   "Daily Work Load Average": "Workload",
                   "Hit target": "HitTarget",
                   "Disciplinary failure": "DisciplineIssue",
                   "Son": "Children",
                   "Social drinker": "SocialDrinker",
                   "Social smoker": "SocialSmoker",
                   "Body Mass Index": "BMI",
                   "Absenteeism Time in Hours": "AbsenceHours"
                  }, inplace=True)

df['year'] = df.index.year
df['month'] = df.index.month
df['day'] = df.index.day
df['day_name'] = df.index.day_name()

# ****
# create additional column to indicate if this record is absent
df['Absent'] = df.AbsenceHours.apply(lambda x:"No" if x != 0 else "Yes") 

# Create a BMI category
df['BMI_category'] = pd.cut(df.BMI,bins = [0,18.4,24.9,29.9, 50], labels=["underweight", "normal", "overweight", "obese"])

df.columns

### 1. Present the numerical and graphical summary for the given variables.
df.loc[:,["TransportExp", "Distance", "ServiceTime", "Workload","HitTarget", "BMI","AbsenceHours" ]].describe()

### 2. Largest variation: Transport Expense. 
# It reflects the high & large spread of transportation cost for workers
df.plot(kind="box", y=["TransportExp", "Distance", "ServiceTime", "Workload","HitTarget", "BMI","AbsenceHours" ], figsize=(50,20),
            fontsize=24)

### 3. What is the period cover in this dataset?
start = df.index.min()
end = df.index.max()
print(f"""Start Date: {start.day}-{start.month}-{start.year}
End Date: {end.day}-{end.month}-{end.year}""")

### 4. Pair up the variables and plot it graphically.
sns.pairplot(df.loc[:,["TransportExp", "Distance", "ServiceTime", "Workload","HitTarget", "BMI","AbsenceHours" ]])

### 5. Examine any association between the variables.
#  All variables appear to be lowly correlated with absent hours.
df.corrwith(df.AbsenceHours).sort_values()

# separating categorical and quantitative variables
category_var = ["Reason", "Seasons", "DisciplineIssue","Education", "Children", "SocialDrinker","SocialSmoker","Pets"]
quan_var = ["TransportExp", "Distance", "ServiceTime","Age","Workload","HitTarget","Weight", "Height","BMI","AbsenceHours"]

# still low correlation
sns.heatmap(df[quan_var].corr(), cmap="Blues")

# assuming the categories are ranked in certain order, still low correlation
df[category_var].corr(method="spearman")

### 6. What is the most common reason for absentees?
# Reason #23, medical consultation
df.Reason.value_counts()[:1]

### 7. Which season do we see the highest absentee rates?
# Season 4-> Spring
df.Seasons.value_counts()[:1]

### 8. Which month do we see the highest absentee rates?
# Absent hour by employee ID and month
pd.pivot_table(df, index=["ID"], columns=["month"], values=["AbsenceHours"]) # Total 36 employees

# Assuming each employee works the same hours -> 8 hrs/day -> 8*22workday=176 hrs
# Assuming total employee numbers stay the same -> 36 -> 36 * 176 work hrs =6336. The supposed total work hours is 6336
sns.heatmap(pd.DataFrame(df.groupby("month").AbsenceHours.sum()/6336*100), cmap="Blues", annot=True)

### 9. Which day of the week do we see the highest absentee rates?
# Monday's absence accounts for 24% of overall absence
# Day of Week in excel =TEXT(DATE, "DDD")
sns.heatmap(pd.DataFrame(df.groupby("day_name").AbsenceHours.sum()/(df.AbsenceHours.sum())), cmap="Blues", annot=True)

### 10. What is the median hours of absence for the most common reason of absence from this dataset?
# Median hour for reason 23 is 2.
df[df.Reason == 23].AbsenceHours.median()

### 11. Do you think BMI plays a part in this issue of absence from work? Based on what evidence?
# Correlation with BMI is only -0.04
df.corrwith(df.AbsenceHours).sort_values()

import matplotlib.pyplot as plt

# Without absence, the most significant category for BMI is Normal.
# Absent, the most significant category for BMI is Obese. 
# Hence absentees have > significant BMI issue as compared to non-absentees.
charts = sns.FacetGrid(df, col="Absent")
charts.map(sns.countplot, 'BMI_category')

# In terms of BMI statistics, no significant difference is observed among these 2 groups of people
sns.boxplot(data=df, y="BMI", x="Absent")

# Plotting out the BMI by reason. The most common reason for absent #23 has larger variation in high BMI range.
charts = sns.FacetGrid(df, col="Reason", col_wrap=5)
charts.map(sns.violinplot, 'BMI')

# 12. Any significant trend or seasonality you can observe from this absentees dataset.
df.ServiceTime.plot() # Examining service time variable

# re-plot by sampling on monthly interval, taking average service time
df.ServiceTime.resample("M").mean().plot()

# sampling on yearly interval
df.ServiceTime.resample("Y").mean().plot()

from statsmodels.tsa.seasonal import seasonal_decompose

decomposed = seasonal_decompose(df.ServiceTime.resample("M").mean(), model="additive")
decomposed.plot() #

from statsmodels.tsa.seasonal import seasonal_decompose

decomposed = seasonal_decompose(df.ServiceTime.resample("M").mean(), model="multiplicative")
decomposed.plot()

# From the above plots, it shows that the serviceTime variable has both trend (mild, not significant)
# and seasonal component repeated on yearly basis.

"""#### Week 5 - Time Series Workshop ------------------------------------ """

import pandas as pd
df_a = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/commodity_a.csv")
df_b = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/commodity_d.csv")
df_c = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/commodity_c.csv")
df_d = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/commodity_d.csv")

import seaborn as sns

# Pair plot showing strong linear correlation btw Silver & Gold
sns.pairplot(df_a.iloc[:,1:])

# Commented out IPython magic to ensure Python compatibility.
from datetime import datetime #%Y-%m-%d %H:%M:%S
"""Other useful directives:
# %a ‐ day of the week (short form)
# %A ‐ day of the week
# %B ‐ January
# %b - Jan
# %Y ‐ year
# %m ‐ month
# %d ‐ day
# %H ‐ hour
# %M ‐ minute
# %S ‐ second"""
df_a['date'] = pd.to_datetime(df_a.Period,
                              format='%b-%y')
df_a.set_index("date")
df_a.plot(kind="scatter", y="Gold", x="date")

df_a.groupby("date").Gold.mean().plot(kind="line")
df_a.groupby("date").Platinum.mean().plot(kind="line")

pd.pivot_table(index="date", data=df_a).plot(kind="line")

pd.pivot_table(index="date", data=df_a.loc[:, ["date", "Gold", "Platinum"]]).plot(kind="line")

pd.pivot_table(index="date", data=df_a.loc[:, ["date", "Silver"]]).plot(kind="line")

df_b.columns = ["Date", "Gold"]
df_b['Period'] = pd.to_datetime(df_b.Period,
                                format='%d/%m/%y')

df_b.plot(kind="line", x="Period", y="GoldETF")

df_c.columns = ["Period", "GoldPrice", "BondYield"]

import matplotlib.pyplot as plt
#df_c
fig, ax = plt.subplots()
ax.scatter(df_c['Period'], df_c['GoldPrice'])

fig, ax = plt.subplots()
ax.scatter(df_c['Period'], df_c['BondYield'])

df_c.groupby("Period").mean().GoldPrice.plot()

df_c.groupby("Period").mean().BondYield.plot()

# Bond Yield is inversely related to bond prices. Bond Yield is a return on
# investment, expressed in % for a bond.
# positive correlation between gold price and bond price.
# The lower the bond price, the higher the yield.
# Hence, negative correlation between gold prices and bond yields.

df_d
df_d.columns = [each.strip() for each in df_d.columns]

df_d[~df_d.Gold.isna()].plot(kind="bar", y="Gold", x="Top Producers")
df_d[~df_d.Silver.isna()].plot(kind="bar", y="Silver", x="Top Producers")
df_d[~df_d.Platinum.isna()].plot(kind="bar", y="Platinum", x="Top Producers")

"""**Task 2 - Employment Rate** ----------------------------------------- """

# Task 2 on Employment Rate
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/employment_rate.csv", parse_dates=[['year', 'month', 'day']], index_col=0)
# 1. Construct the plots for low, middle and high income groups over months.
fig, ax = plt.subplots(1, 1)
ax.plot(df.index.month, df.emp_combined_incmiddle, label="Middle Income")
ax.plot(df.index.month,df.emp_combined_inclow, label="Low Income")
ax.plot(df.index.month,df.emp_combined_inchigh, label="High Income")
ax.legend()
ax.set_title("Employment Rates Changes in First 8 months of 2020")

"""
Notice the graph above showing 2 values for each month, the 2 values denote
the minimum and maximum within the give month. 
See the following details in number for low income group (Orange color form the above plot)
"""
df.emp_combined_inclow.groupby(df.index.month).describe()

from statsmodels.tsa.stattools import *
# 2. Do they exhibit significant trend or seasonality?
# Unit root = 1, non-stationary
result = adfuller(df.emp_combined)
print(f"Test Statistic: {result[0]}, p-value: {result[1]}") # ADF statistic and p-value.
"""
Null Hypothesis-> There is a unit root 
p-value
In this case, the test statistic is -3.09969
p-value is 0.0265 > 0.01 for 1% significance level, 
Hence null hypothesis that there is a unit root cannot be rejected 
so the data is non-stationary implying the presence of trend/seasonality
In the event of higher significance level, 
e.g. 5%, the null hypothesis will then be rejected and hence stationary.
"""

decomposed = seasonal_decompose(df.emp_combined, model="additive")

# The following plots display the trend and seasonal components found in emp_combined variable.
decomposed.plot()

# Which month has the lowest employment rate?
# April
df[df.emp_combined == df.emp_combined.min()].index.month_name()

# For emp_combined variable, which decomposition method is more appropriate? Why?
# decomposed = seasonal_decompose(df.emp_combined, model="multiplicative")
# decomposed.plot() 
# The above will result in error due to the presence of negative values.
# Hence, additive model is more suitable in this case.

"""**visitors.csv - autocorrelation and moving average (MA)**"""

import pandas as pd
from matplotlib import pyplot as plt
df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/visitors.csv", index_col=0)
df.head(3)

from pandas.plotting import lag_plot
lag_plot(df)

lag_plot(df, lag=2)

from pandas.plotting import *
autocorrelation_plot(df)

import pandas as pd
data = pd.DataFrame({
    "time_data": [i for i in range(10)]})
data

data.rolling(window=2).mean()

data.ewm(span=2).mean()

import pandas as pd
from matplotlib import pyplot as plt

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/visitors.csv", index_col=0)
df.head(3)

df = df.iloc[:20]
df.shape

sma2 = df.rolling(window=2)

df2 = sma2.mean()

# getting mean for simple moving average
f2 = sma2.mean()

import matplotlib.pyplot as plt
plt.figure(figsize=(20,10))
plt.plot(df.index, df['Sydney'], color = 'blue', label='original data')
plt.plot(df.index, df2['Sydney'], color = 'red', label='SMA2')

"""#### Workshop - Airbnb.CSV"""

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/airbnb_toClean.csv")
df.head()

# Plotting using df.plot(kind="..")
df.plot(kind="density", y="price")

df.plot(kind="box", y="price")

df_outliers  = df[df.price  > 400]

df.plot(kind="hist", y="price")

df.plot(kind="hist", y="price", bins=100)

# Plotting using seaborn
import seaborn as sns

# plot average/mean price for room_type
sns.barplot(data=df, y="price", x="room_type", ci=None)

from numpy import median

# plot median price for room_type
sns.barplot(data=df, y="price", x="room_type",
            estimator=median, ci=None)

# plot price for minimum nights less than 14, per night
sns.barplot(data=df[df.minimum_nights < 14], y="price",
            x="minimum_nights",
            hue="room_type", ci=None)

# plot minimum nights (<10) price per room_type
df[df.minimum_nights < 10].pivot_table(index="room_type",columns="minimum_nights",
               values="price").plot(kind="bar")

# plot minimum nights <10 per room type
df1 = df[df.minimum_nights < 10]
pd.crosstab(df1.room_type, df1.minimum_nights).plot(kind="bar")

sns.histplot(data=df, x="price")

sns.boxplot(data=df, y="price", x="room_type")

import matplotlib.pyplot as plt

# FacetGrid Box Plot
charts = sns.FacetGrid(df, col="room_type")
charts.map(sns.boxplot, "price")

charts = sns.FacetGrid(df, col="room_type")

# FacetGrid Violin Plot
charts.map(sns.violinplot, "price")

charts = sns.FacetGrid(df, col="room_type")

# FacetGrid Histogram
charts.map(plt.hist, "price")

df[df.room_type == "Shared room"]

import plotly.express as px

fig = px.scatter(df, x="price", y="number_of_reviews")
fig.show()

# Pie Chart

fig = px.pie(df, values="number_of_reviews",
             names="minimum_nights")
fig.show()

# Sunburst Chart

fig = px.sunburst(df, path=["room_type"], values="price")
fig.show()

df.head()

df['area'] = df.neighbour_hood_info.apply(lambda x:x.split(",")[0])
df.head()

# Sunburst for area and room_type
fig = px.sunburst(df, path=["area", "room_type"], values="price")
fig.show()

"""#### Frauda.csv ------------------------------------------------------ """

import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/fraud_parta.csv")
df.head()

df.type.unique()

# ['PAYMENT', 'TRANSFER', 'CASH_OUT', 'DEBIT', 'CASH_IN']
df['type'] = df.type.replace({
    'PAYMENT': 0,
    'TRANSFER': 1,
    'CASH_OUT': 2,
    'DEBIT': 3,
    'CASH_IN': 4
})
df.type.head()

df.head()

df.type.unique()

df.drop(['step','nameOrig', 'nameDest', 'isFlaggedFraud'],
        axis=1, inplace=True)
df.head()

x = df.iloc[:, :-1] # select all cols except last
y = df.iloc[:, -1] # select the last col

y.head()

"""#### Week 7: Support Vector Machine (SVM) ---------------------------- """

# standardize or data scaling
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
x_transformed = ss.fit_transform(x)

x_transformed

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x_transformed, y, test_size=0.3, random_state=0
)

# classifier
from sklearn.svm import SVC

classifer = SVC(kernel="linear", random_state=0) # create model
classifer.fit(x_train, y_train)

y_pred = classifer.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)
cm

report = classification_report(y_test, y_pred)
print(report)

"""#### Week 9: K-Means Clustering & Hierarchy Clustering --------------- """

import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/yokyen/data/master/bankcustomers.csv")

df.head()

from sklearn.cluster import KMeans

x = df.loc[:, ['debt_equity', 'income', 'age']]

model = KMeans(n_clusters=3)
model.fit(x)

model.predict(x)

df['cluster'] = model.predict(x)

df.groupby('cluster').mean()

import seaborn

seaborn.lmplot(x="debt_equity", y="income", data=df, fit_reg=False, hue="cluster")

seaborn.lmplot(x="debt_equity", y="age", data=df, fit_reg=False, hue="cluster")

seaborn.lmplot(x="age", y="income", data=df, fit_reg=False, hue="cluster")

"""#### Week 10: Text Mining and NLP ------------------------------------ """

import pandas as pd

df = pd.read_csv("news.csv")

df.head()

from nltk.sentiment.vader import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

df.iloc[0, -1]

analyzer.polarity_scores(df.iloc[0, -1])

positive_news = []
negative_news = []
neutral_news = []

for index, row in df.iterrows():
    score = analyzer.polarity_scores(row['clean_text'])
    if score['compound'] < -0.05:
        negative_news.append(row['clean_text'])
    elif score['compound'] > 0.05:
        positive_news.append(row['clean_text'])
    else:
        neutral_news.append(row['clean_text'])
    print(f"{score['compound']} -> {row['clean_text']}")

print("Positive news:", len(positive_news))
print("Negative news:", len(negative_news))
print("Neutral news:", len(neutral_news))

with open("restaurant.txt", "r") as f:
    reviews = f.readlines()

for index, data in enumerate(reviews):
    print(index, data.strip())

positive_reviews = []
negative_reviews = []
neutral_reviews = []

for index, data in enumerate(reviews):
    score = analyzer.polarity_scores(data)
    if score['compound'] < -0.05:
        negative_reviews.append(data)
    elif score['compound'] > 0.05:
        positive_reviews.append(data)
    else:
        neutral_reviews.append(data)
    print(f"{score['compound']} -> {data}")

print("Positive reviews:", len(positive_reviews))
print("Negative reviews:", len(negative_reviews))
print("Neutral reviews:", len(neutral_reviews))

with open("restaurant.txt", "r") as f:
    reviews = f.read()

reviews

from wordcloud import WordCloud
import matplotlib.pyplot as plt

wordcloud = WordCloud(max_font_size=40).generate(reviews)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")

import pandas as pd
from sklearn.neural_network import MLPRegressor
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

df = pd.read_csv("churning.csv")
# we use only the first 2 variables as features
# account length,number vmail messages
x = df.iloc[:, [0, 1]]
y = df.iloc[:,-1] # classification
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)
classifier = AdaBoostClassifier(n_estimators=10, learning_rate=0.8, random_state=0, algorithm='SAMME')
classifier.fit(x, y)
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(accuracy_score(y_test, y_pred))
while True:
  var1 = float(input("Enter value for account length:"))
  var2 = float(input("Enter value for number vmail messages:"))
  pred = classifier.predict([[var1, var2]])
  print(pred)
  if pred == [1]:
      input("Yes, likely will churn.")
  else:
      input("Unlikely will churn.")
  cont = input("Do you want to continue? y/n")
  if cont == "n":
      break

import pickle
with open("my_ml_model1.ml", "wb") as f:
    pickle.dump(classifer, f)

# Create an object file called "my_ml_model1.ml" keeping the state of classifier